# crawline

__TODO:__ バッヂ

> Webクローリングとスクレイピングを行う基盤アプリケーション

Webページをデータソースとするアプリケーションに向けて、当基盤がWebクローリングとスクレイピングを行い、その結果のデータを提供する。当基盤にルールを登録しておいて、URLをパラメータとしてクローリングを要求すると、ルールに沿ってクローリングとスクレイピングを行う。ダウンロードしたデータの管理、キャッシュ管理などは当基盤が行い、個別のアプリケーションはデータモデルの構築に専念できる。

__Table of Contents__

<!-- TOC depthFrom:2 -->

- [Security](#security)
- [Background](#background)
- [Install](#install)
- [Usage](#usage)
  - [スクレイピングのルールを定義する](#スクレイピングのルールを定義する)
  - [URLを起点にクロール、スクレイピングを行う](#urlを起点にクロールスクレイピングを行う)
- [API](#api)
- [Maintainer](#maintainer)
- [Contribute](#contribute)
- [License](#license)

<!-- /TOC -->

## Security

個人用のアプリケーションであり、複数人数が使用することは想定していない。外部からはジョブ・スケジューラーでジョブを実行するだけ。

そのため、認証機能は実装しない。nginx-proxyの機能でBASIC認証を実装しても良いし、Let's Encryptでサーバー証明書を設定しても良いが、それは外部アプリケーションの役割とする。

## Background

いくつかの似たようなスクレイピング・アプリケーションを作成してきた。新しいサイトのスクレイピング要件はこれからも出続けるだろうし、その度に新しいアプリケーションを作るのは、時間とマシン・リソースの無駄。

そのため、クローリングとスクレイピングの基盤を構築して、複数のサイトに対するスクレイピングを統合したい。ページに対する解析方法、検証方法、リンク解析方法などを、コードで制御して、気軽に追加・変更できるようにしたい。ページのキャッシュ制御、旧バージョンの管理なども任せたい。

解析後のデータのモデル構築などは、個々のアプリケーションがやるべきだと思っているので、ここではやらない。

## Install

__TODO:__ 基盤コードとルール・コードの管理を分けたい。そのため、gemによる提供を考える

## Usage

### スクレイピングのルールを定義する

__TODO:__ 目的、具体的な手順、例を示す

ページに対する解析、検証、リンク抽出方法は、クラスで定義する。XPathで定義とも考えたが、それだと複雑なページが解析できない気がする。

ダウンロードしたページの保存、キャッシュ制御は基盤が自動的に行う。RFC的なキャッシュ制御のほか、「以前のダウンロードから1か月はキャッシュを使う」などの優先独自ルールを設定可能とする。

クローリング時間間隔、異なるドメインへの平行ダウンロードなど、基盤が制御する。

### URLを起点にクロール、スクレイピングを行う

__TODO:__ 目的、具体的な手順、例を示す

外部からクロールして欲しいURLを指定すると、ルールに従って解析して、リンクを辿れるだけ辿る。

## API

__TODO:__ APIドキュメントへのリンクを示す

## Maintainer

- u6k
  - [Twitter](https://twitter.com/u6k_yu1)
  - [GitHub](https://github.com/u6k)
  - [Blog](https://blog.u6k.me/)

## Contribute

当プロジェクトに興味を持っていただき、ありがとうございます。[新しいチケットを起票](https://redmine.u6k.me/projects/crawline/issues/new)していただくか、プルリクエストをサブミットしていただけると幸いです。

当プロジェクトは、[Contributor Covenant](https://www.contributor-covenant.org/version/1/4/code-of-conduct)に準拠します。

## License

[MIT License](https://github.com/u6k/crawline/blob/master/LICENSE)
